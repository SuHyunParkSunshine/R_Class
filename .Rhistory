# 실습: 단순 선형 회귀분석 수행
# 단계 1: 데이터 가져오기
product <- read.csv("./data/product.csv", header = TRUE, fileEncoding = "euc-kr")
str(product)
str(product)
# 단계 2: 독립변수와 종속벼수 생성
y = product$제품_만족도
x = product$제품_적절성
df <- data.frame(x, y)
# 단계 3: 단순 선형회귀 모델 생성
result.lm <- lm(formula = y ~ x, data = df) #lm(linear)
# 단계 4: 회귀분석의 절편과 기울기
result.lm
# 단계 5: 모델의 적합값과 잔차 보기
names(result.lm)
# 단계 5-1: 적합값 보기
fitted.values(result.lm)[1:2]
# 단계 5-2: 관측값 보기
head(df, 1)
# 단계 5-3: 회귀방정식을 적용하여 모델의 적합값 계산
Y = 0.7789 + 0.7393 * 4
Y
# 단계 5-4: 잔차(오차) 계산
3 - 3.735963
# 단계 5-5: 모델의 잔차 보기
residuals(result.lm)[1:2]
# 단계 5-6: 모델의 잔차와 회귀방정식에 의한 적합값으로부터 관측값 계산
-0.7359630 + 3.735963
# 실습: 선형 회귀분석 모델 시각화
# 단계 1: x, y 산점도 그리기
plot(formula = y ~ x, data = product)
# 단계 2: 선형 회귀모델 생성
result.lm <- lm(formula = y ~ x, data = product)
# 단계 3: 회귀선
abline(result.lm, col = "red")
# 실습: 날씨 관련 요인 변수로 비(rain) 유뮤 예측
# 단계 1: 데이터 가져오기
weather = read.csv("./data/weather.csv", stringsAsFactors = F)
dim(weather)
head(weather)
str(weather)
# 단계 3: 학습데이터와 검정데이터 생성(7:3 비율)
idx <- sample(1:nrow(weather_df), nrow(weather_df) * 0.7)
train <- weather_df[idx, ]
test <- weather_df[-idx, ]
# 단계 2: 변수 선택과 더비 벼수 생성
weather_df <- weather[ , c(-1, -6, -8, -14)]
str(weather_df)
weather_df$RainTomorrow[weather_df$RainTomorrow == 'Yes'] <- 1
weather_df$RainTomorrow[weather_df$RainTomorrow == 'No'] <- 0
weather_df$RainTomorrow <- as.numeric(weather_df$RainTomorrow)
head(weather_df)
# 단계 3: 학습데이터와 검정데이터 생성(7:3 비율)
idx <- sample(1:nrow(weather_df), nrow(weather_df) * 0.7)
train <- weather_df[idx, ]
test <- weather_df[-idx, ]
# 단계 4: 로지스틱 회귀모델 생성 (로지스틱 회귀모델은 '분류모델')
weather_model <- glm(RainTomorrow ~ ., data = train, family = 'binomial')
weather_model
summary(weather_model)
# 단계 5: 로지스틱 회귀모델 예측치 생성
pred <- predict(weather_model, newdata = test, type = "response")
pred
# 시그모이드 함수
result_pred <- ifelse(pred >= 0.5, 1, 0)
result_pred
table(result_pred)
# 단계 6: 모델 평가 - 분류정확도 계산
table(result_pred, test$RainTomorrow)
# 단계 7: ROC Curve를 이용한 모델 평가
library(ROCR)
pr <- prediction(pred, test$RainTomorrow)
prf <- performance(pr, measure = "tpr", x.maeasure = "fpr")
# 실습: 의사결정 트리 생성: ctree() 함수 이용
# 단계 1: party 패키지 설치
library(party)
install.packages("party")
# 실습: 의사결정 트리 생성: ctree() 함수 이용
# 단계 1: party 패키지 설치
library(party)
# 단계 2: airquality 데이터 셋 로딩
#install.packages("datasets")
library(datasets)
str(airquality)
# 단계 3: formula 생성
formula <- Temp ~ Solar.R + Wind + Ozone
# 단계 4: 분류모델 생성 - formula를 이용하여 분류모델 생성
air_ctree <- ctree(formula, data = airquality)
air_ctree
# 단계 5: 분류분석 결과
plot(air_ctree)
install.packages("tidyverse")
install.packages("tidymodels")
install.packages("tictoc")
install.packages("doParallel")
install.packages("furrr")
install.packages("ranger")
install.packages("glmnet")
install.packages("palmerpenguins")
install.packages("hrbrthemes")
##패키지 불러오기
library(tidyverse)
library(tidymodels)
library(tictoc)
library(doParallel)
library(furrr)
library(ranger)
library(glmnet)
library(palmerpenguins)
library(hrbrthemes)
hrbrthemes::import_roboto_condensed()
##환경설정
theme_set(hrbrthemes::theme_ipsum_rc())
plan(multicore, workers = availableCores())
cores <- parallel::detectCores(logical = FALSE)
cl <- makePSOCKcluster(cores)
registerDoParallel(cores = cl)
set.seed(42)
# 1. 데이터 불러오기 및 전처리
penguins_data <- palmerpenguins::penguins
# 1. 데이터 불러오기 및 전처리
penguins_data <- palmerpenguins::penguins #(패키지명 :: 안에 있는 내용) 가져와져
penguins_data
#NA값 부터 날리기
glimpse(penguins_data)
t(map_df(penguins_data, ~sum(is.na(.))))
penguins_df <-
penguins_data
t(map_df(penguins_data, ~sum(is.na(.))))
penguins_df <-
penguins_data %>%
filter(!is.na(sex)) %>%
select(-year, -island)
#NA값 부터 날리기
glimpse(penguins_data)
# 1. 데이터 불러오기 및 전처리
penguins_data <- palmerpenguins::penguins #(패키지명 :: 안에 있는 내용) 가져와져
##패키지 불러오기
library(tidyverse)
library(tidymodels)
library(tictoc)
library(doParallel)
library(furrr)
library(ranger)
library(glmnet)
library(palmerpenguins)
library(hrbrthemes)
hrbrthemes::import_roboto_condensed()
##환경설정
theme_set(hrbrthemes::theme_ipsum_rc()) # --> 배경 흰색으로 설정
plan(multicore, workers = availableCores())
cores <- parallel::detectCores(logical = FALSE)
cl <- makePSOCKcluster(cores)
registerDoParallel(cores = cl)
set.seed(42)  # random결과를 seed값 42 기준으로 가져옴.시드값 가져오면 아무리 데이터 셔플링해도 동일한 셔플링 진행
# 1. 데이터 불러오기 및 전처리
penguins_data <- palmerpenguins::penguins #(패키지명 :: 안에 있는 내용) 가져와져
#penguins_data <- penguins #패키지 위에서 불러온 경우, penguins만 불러도 가져와짐
penguins_data
#NA값 부터 날리기
glimpse(penguins_data)
t(map_df(penguins_data, ~sum(is.na(.))))
penguins_df <-
penguins_data %>%
filter(!is.na(sex)) %>%
select(-year, -island)
head(penguins_df)
penguins_split <-
rsample::initial_split(
penguins_df,
prop = 0.7,
strata = species #종 기준으로 적절하게 나눠줘
)
## 2. 기준(Baseline Experiment) 설정
tic(" Baseline XGBoost training duration ")
xgboost_fit <-
boost_tree() %>%
set_engine("xgboost") %>%
set_mode("classification") %>%
fit(species ~ ., data = training(penguins_split))
## 2. 기준(Baseline Experiment) 설정/ 베이스라인 구축
install.packages("xgboost")
tic(" Baseline XGBoost training duration ")
xgboost_fit <-
boost_tree() %>% #boost 알고리즘
set_engine("xgboost") %>%
set_mode("classification") %>% #분류모델을 써라
fit(species ~ ., data = training(penguins_split))
preds <- predict(xgboost_fit, new_data = testing(penguins_split))
tic(" Baseline XGBoost training duration ")
xgboost_fit <-
boost_tree() %>% #boost 알고리즘
set_engine("xgboost") %>%
set_mode("classification") %>% #분류모델을 써라
fit(species ~ ., data = training(penguins_split))
toc(log = TRUE)
preds <- predict(xgboost_fit, new_data = testing(penguins_split))
actual <- testing(penguins_split) %>% select(species)
yardstick::f_meas_vec(truth = actual$species, estimate = preds$.pred_class)
## 3. 모델 설정
#-- model 1
ranger_model <-
parsnip::rand_forest(mtry = tune(), min_n = tune()) %>% #'rand_forest' 학자들이 만든 것. 'min_n': 최소 몇 가지 내려갈건데?
set_engine("ranger") %>%
set_mode("classification")
#-- model 2
glm_model <- #제약조건을 기반으로 penalty 값을 준다. tune은 내가 할 생각이 없는 것
parsnip::multinom_reg(penalty = tune(), mixture = tune()) %>%
set_engine("glmnet") %>%
set_mode("classification")
#-- model 3
xgboost_model <-
parsnip::boost_tree(mtry = tune(), learn_rate = tune()) %>%
set_engine("xgboost") %>%
set_mode("classification")
hardhat::extract_parameter_dials(glm_model, "mixture")
## 4. Grid 검색(전역 검색)
ranger_grid <-
hardhat::extract_parameter_set_dials(ranger_model) %>%
finalize(select(training(penguins_split), -species)) %>%
grid_regular(levels = 4)
ranger_grid
ranger_grid %>% ggplot(aes(mtry, min_n)) +
geom_point(size = 4, alpha = 0.6) +
labs(title = "Ranger: Regular grid for min_n & mtry combinations")
glm_grid <-
parameters(glm_model) %>%
grid_random(size = 20)
glm_grid
glm_grid %>% ggplot(aes(penalty, mixture)) +
geom_point(size = 4, alpha = 0.6) +
labs(title = "GLM: Random grid for penalty & mixture combinations")
xgboost_grid <-
parameters(xgboost_model) %>%
finalize(select(training(penguins_split), -species)) %>%
grid_max_entropy(size = 20)
xgboost_grid
xgboost_grid %>% ggplot(aes(mtry, learn_rate)) +
geom_point(size = 4, alpha = 0.6) +
labs(title = "XGBoost: Max Entropy grid for LR & mtry combinations")
## 5. 데이터 전처리
recipe_base <-
recipe(species ~ ., data = training(penguins_split)) %>%
step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) # Create dummy variables (which glmnet needs)
recipe_1 <-
recipe_base %>%
step_YeoJohnson(all_numeric())
recipe_1 %>%
prep() %>%
juice() %>%
summary()
recipe_2 <-
recipe_base %>%
step_normalize(all_numeric())
recipe_2 %>%
prep() %>%
juice() %>%
summary()
recipe_2 <-
recipe_base %>%
step_normalize(all_numeric())
recipe_2 %>%
prep() %>%
juice() %>%
summary()
## 6.  Metrics
model_metrics <- yardstick::metric_set(f_meas, pr_auc)
## 7. K-Fold CV
data_penguins_3_cv_folds <-
rsample::vfold_cv(
v = 5,
data = training(penguins_split),
strata = species
)
## 8. 모델 학습(Model Training)
## 8 - 1. 일괄 작업 작성
ranger_r1_workflow <-
workflows::workflow() %>%
add_model(ranger_model) %>%
add_recipe(recipe_1)
glm_r2_workflow <-
workflows::workflow() %>%
add_model(glm_model) %>%
add_recipe(recipe_2)
xgboost_r2_workflow <-
workflows::workflow() %>%
add_model(xgboost_model) %>%
add_recipe(recipe_2)
## 8 - 2.  Gridsearch를 활용한 학습
tic("Ranger tune grid training duration ")
ranger_tuned <-
tune::tune_grid(
object = ranger_r1_workflow,
resamples = data_penguins_3_cv_folds,
grid = ranger_grid,
metrics = model_metrics,
control = tune::control_grid(save_pred = TRUE)
)
toc(log = TRUE)
tic("GLM tune grid training duration ")
glm_tuned <-
tune::tune_grid(
object = glm_r2_workflow,
resamples = data_penguins_3_cv_folds,
grid = glm_grid,
metrics = model_metrics,
control = tune::control_grid(save_pred = TRUE)
)
toc(log = TRUE)
tic("XGBoost tune grid training duration ")
xgboost_tuned <-
tune::tune_grid(
object = xgboost_r2_workflow,
resamples = data_penguins_3_cv_folds,
grid = xgboost_grid,
metrics = model_metrics,
control = tune::control_grid(save_pred = TRUE)
)
toc(log = TRUE)
#결과확인용
install.packages("finetune")
## 8 - 3. 학습 결과 확인
library(finetune)
tic("Tune race training duration ")
ft_xgboost_tuned <-
finetune::tune_race_anova(
object = xgboost_r2_workflow,
resamples = data_penguins_3_cv_folds,
grid = xgboost_grid,
metrics = model_metrics,
control = control_race(verbose_elim = TRUE) # 66
)
toc(log = TRUE)
plot_race(ft_xgboost_tuned) + labs(title = "Parameters Race by Fold")
# 결과
bind_cols(
tibble(model = c("Ranger", "GLM", "XGBoost")),
bind_rows(
ranger_tuned %>%
collect_metrics() %>% group_by(.metric) %>% summarise(best_va = max(mean, na.rm = TRUE)) %>% arranglm_tuned %>%
glm_tuned %>%
collect_metrics() %>% group_by(.metric) %>% summarise(best_va = max(mean, na.rm = TRUE)) %>% arranglm_tuned %>%
xgboost_tuned %>%
collect_metrics() %>% group_by(.metric) %>% summarise(best_va = max(mean, na.rm = TRUE)) %>% arran)
)
## 0. 패키지 불러오기
library(tidyverse)
## 1. 데이터 프레임 작성
# 파일 목록을 다 들고 와야 됨
files <- list.files(path = "data/nasdaq_stock/")
# 들고온 파일 목록을 다 읽어서, 데이터프레임
read_csv(paste0("data/nasdaq_stock/", files))
# 들고온 파일 목록을 다 읽어서, 데이터프레임
read_csv(paste0("data/nasdaq_stock/", files), id = "name")
# 들고온 파일 목록을 다 읽어서, 데이터프레임
read_csv(paste0("data/nasdaq_stock/", files), id = "name") %>% # %>% 이 기호 나오면 tidyverse 쓸거임
mutate(name = gsub("data/nasdaq_stock/", "", name))
# 들고온 파일 목록을 다 읽어서, 데이터프레임
read_csv(paste0("data/nasdaq_stock/", files), id = "name") %>% # %>% 이 기호 나오면 tidyverse 쓸거임
mutate(name = gsub("data/nasdaq_stock/", "", name),
name = gsub("\\.csv", "", name)) %>%
rename_with((tolower))
stocks
# 들고온 파일 목록을 다 읽어서, 데이터프레임
stocks <- read_csv(paste0("data/nasdaq_stock/", files), id = "name") %>% # %>% 이 기호 나오면 tidyverse 쓸거임
mutate(name = gsub("data/nasdaq_stock/", "", name),
name = gsub("\\.csv", "", name)) %>%
rename_with((tolower))
stocks
# 들고온 파일 목록을 다 읽어서, 데이터프레임
stocks <- read_csv(paste0("data/nasdaq_stock/", files), id = "name") %>% # %>% 이 기호 나오면 tidyverse 쓸거임
mutate(name = gsub("data/nasdaq_stock/", "", name),
name = gsub("\\.csv", "", name)) %>%
rename_with(tolower)
stocks
# 데이터 프레임을 결합
df <- read_csv("data/nasdaq_stock_names.csv")
df
stocks %>%
inner_join(df, by = c("name" = "stock_symbol"))
stocks <-
stocks %>%
inner_join(df, by = c("name" = "stock_symbol"))
stocks
View(stocks)
## 2. 시계열 데이터 시각화
stocks %>%
group_by(company)
## 2. 시계열 데이터 시각화
stocks %>%
group_by(company)
## 2. 시계열 데이터 시각화
(stocks %>%
group_by(company) %>%
filter(date == max(data)) %>%
arrange(-open) %>%
select(open, company))[c(1:3, 12:14),]
## 2. 시계열 데이터 시각화
(stocks %>%
group_by(company) %>%
filter(date == max(date)) %>%
arrange(-open) %>%
select(open, company))[c(1:3, 12:14),]
## 2. 시계열 데이터 시각화
(stocks %>%
group_by(company) %>%
filter(date == max(date)) %>%
arrange(-open) %>%  # 시작가 시준으로 sorting을 다시함
select(open, company)) #[c(1:3, 12:14),] #1~3번째, 12~14번째 가지고 올거얌
stocks %>%
ggplot(aes(date, open)) +
geom_line(ase(color = company))
stocks %>%
ggplot(aes(date, open)) +
geom_line(aes(color = company))
