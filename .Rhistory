# 단계 3: 학습데이터와 검정데이터 생성(7:3 비율)
idx <- sample(1:nrow(weather_df), nrow(weather_df) * 0.7)
train <- weather_df[idx, ]
test <- weather_df[-idx, ]
# 단계 2: 변수 선택과 더비 벼수 생성
weather_df <- weather[ , c(-1, -6, -8, -14)]
str(weather_df)
weather_df$RainTomorrow[weather_df$RainTomorrow == 'Yes'] <- 1
weather_df$RainTomorrow[weather_df$RainTomorrow == 'No'] <- 0
weather_df$RainTomorrow <- as.numeric(weather_df$RainTomorrow)
head(weather_df)
# 단계 3: 학습데이터와 검정데이터 생성(7:3 비율)
idx <- sample(1:nrow(weather_df), nrow(weather_df) * 0.7)
train <- weather_df[idx, ]
test <- weather_df[-idx, ]
# 단계 4: 로지스틱 회귀모델 생성 (로지스틱 회귀모델은 '분류모델')
weather_model <- glm(RainTomorrow ~ ., data = train, family = 'binomial')
weather_model
summary(weather_model)
# 단계 5: 로지스틱 회귀모델 예측치 생성
pred <- predict(weather_model, newdata = test, type = "response")
pred
# 시그모이드 함수
result_pred <- ifelse(pred >= 0.5, 1, 0)
result_pred
table(result_pred)
# 단계 6: 모델 평가 - 분류정확도 계산
table(result_pred, test$RainTomorrow)
# 단계 7: ROC Curve를 이용한 모델 평가
library(ROCR)
pr <- prediction(pred, test$RainTomorrow)
prf <- performance(pr, measure = "tpr", x.maeasure = "fpr")
# 실습: 의사결정 트리 생성: ctree() 함수 이용
# 단계 1: party 패키지 설치
library(party)
install.packages("party")
# 실습: 의사결정 트리 생성: ctree() 함수 이용
# 단계 1: party 패키지 설치
library(party)
# 단계 2: airquality 데이터 셋 로딩
#install.packages("datasets")
library(datasets)
str(airquality)
# 단계 3: formula 생성
formula <- Temp ~ Solar.R + Wind + Ozone
# 단계 4: 분류모델 생성 - formula를 이용하여 분류모델 생성
air_ctree <- ctree(formula, data = airquality)
air_ctree
# 단계 5: 분류분석 결과
plot(air_ctree)
install.packages("tidyverse")
install.packages("tidymodels")
install.packages("tictoc")
install.packages("doParallel")
install.packages("furrr")
install.packages("ranger")
install.packages("glmnet")
install.packages("palmerpenguins")
install.packages("hrbrthemes")
##패키지 불러오기
library(tidyverse)
library(tidymodels)
library(tictoc)
library(doParallel)
library(furrr)
library(ranger)
library(glmnet)
library(palmerpenguins)
library(hrbrthemes)
hrbrthemes::import_roboto_condensed()
##환경설정
theme_set(hrbrthemes::theme_ipsum_rc())
plan(multicore, workers = availableCores())
cores <- parallel::detectCores(logical = FALSE)
cl <- makePSOCKcluster(cores)
registerDoParallel(cores = cl)
set.seed(42)
# 1. 데이터 불러오기 및 전처리
penguins_data <- palmerpenguins::penguins
# 1. 데이터 불러오기 및 전처리
penguins_data <- palmerpenguins::penguins #(패키지명 :: 안에 있는 내용) 가져와져
penguins_data
#NA값 부터 날리기
glimpse(penguins_data)
t(map_df(penguins_data, ~sum(is.na(.))))
penguins_df <-
penguins_data
t(map_df(penguins_data, ~sum(is.na(.))))
penguins_df <-
penguins_data %>%
filter(!is.na(sex)) %>%
select(-year, -island)
#NA값 부터 날리기
glimpse(penguins_data)
# 1. 데이터 불러오기 및 전처리
penguins_data <- palmerpenguins::penguins #(패키지명 :: 안에 있는 내용) 가져와져
##패키지 불러오기
library(tidyverse)
library(tidymodels)
library(tictoc)
library(doParallel)
library(furrr)
library(ranger)
library(glmnet)
library(palmerpenguins)
library(hrbrthemes)
hrbrthemes::import_roboto_condensed()
##환경설정
theme_set(hrbrthemes::theme_ipsum_rc()) # --> 배경 흰색으로 설정
plan(multicore, workers = availableCores())
cores <- parallel::detectCores(logical = FALSE)
cl <- makePSOCKcluster(cores)
registerDoParallel(cores = cl)
set.seed(42)  # random결과를 seed값 42 기준으로 가져옴.시드값 가져오면 아무리 데이터 셔플링해도 동일한 셔플링 진행
# 1. 데이터 불러오기 및 전처리
penguins_data <- palmerpenguins::penguins #(패키지명 :: 안에 있는 내용) 가져와져
#penguins_data <- penguins #패키지 위에서 불러온 경우, penguins만 불러도 가져와짐
penguins_data
#NA값 부터 날리기
glimpse(penguins_data)
t(map_df(penguins_data, ~sum(is.na(.))))
penguins_df <-
penguins_data %>%
filter(!is.na(sex)) %>%
select(-year, -island)
head(penguins_df)
penguins_split <-
rsample::initial_split(
penguins_df,
prop = 0.7,
strata = species #종 기준으로 적절하게 나눠줘
)
## 2. 기준(Baseline Experiment) 설정
tic(" Baseline XGBoost training duration ")
xgboost_fit <-
boost_tree() %>%
set_engine("xgboost") %>%
set_mode("classification") %>%
fit(species ~ ., data = training(penguins_split))
## 2. 기준(Baseline Experiment) 설정/ 베이스라인 구축
install.packages("xgboost")
tic(" Baseline XGBoost training duration ")
xgboost_fit <-
boost_tree() %>% #boost 알고리즘
set_engine("xgboost") %>%
set_mode("classification") %>% #분류모델을 써라
fit(species ~ ., data = training(penguins_split))
preds <- predict(xgboost_fit, new_data = testing(penguins_split))
tic(" Baseline XGBoost training duration ")
xgboost_fit <-
boost_tree() %>% #boost 알고리즘
set_engine("xgboost") %>%
set_mode("classification") %>% #분류모델을 써라
fit(species ~ ., data = training(penguins_split))
toc(log = TRUE)
preds <- predict(xgboost_fit, new_data = testing(penguins_split))
actual <- testing(penguins_split) %>% select(species)
yardstick::f_meas_vec(truth = actual$species, estimate = preds$.pred_class)
## 3. 모델 설정
#-- model 1
ranger_model <-
parsnip::rand_forest(mtry = tune(), min_n = tune()) %>% #'rand_forest' 학자들이 만든 것. 'min_n': 최소 몇 가지 내려갈건데?
set_engine("ranger") %>%
set_mode("classification")
#-- model 2
glm_model <- #제약조건을 기반으로 penalty 값을 준다. tune은 내가 할 생각이 없는 것
parsnip::multinom_reg(penalty = tune(), mixture = tune()) %>%
set_engine("glmnet") %>%
set_mode("classification")
#-- model 3
xgboost_model <-
parsnip::boost_tree(mtry = tune(), learn_rate = tune()) %>%
set_engine("xgboost") %>%
set_mode("classification")
hardhat::extract_parameter_dials(glm_model, "mixture")
## 4. Grid 검색(전역 검색)
ranger_grid <-
hardhat::extract_parameter_set_dials(ranger_model) %>%
finalize(select(training(penguins_split), -species)) %>%
grid_regular(levels = 4)
ranger_grid
ranger_grid %>% ggplot(aes(mtry, min_n)) +
geom_point(size = 4, alpha = 0.6) +
labs(title = "Ranger: Regular grid for min_n & mtry combinations")
glm_grid <-
parameters(glm_model) %>%
grid_random(size = 20)
glm_grid
glm_grid %>% ggplot(aes(penalty, mixture)) +
geom_point(size = 4, alpha = 0.6) +
labs(title = "GLM: Random grid for penalty & mixture combinations")
xgboost_grid <-
parameters(xgboost_model) %>%
finalize(select(training(penguins_split), -species)) %>%
grid_max_entropy(size = 20)
xgboost_grid
xgboost_grid %>% ggplot(aes(mtry, learn_rate)) +
geom_point(size = 4, alpha = 0.6) +
labs(title = "XGBoost: Max Entropy grid for LR & mtry combinations")
## 5. 데이터 전처리
recipe_base <-
recipe(species ~ ., data = training(penguins_split)) %>%
step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) # Create dummy variables (which glmnet needs)
recipe_1 <-
recipe_base %>%
step_YeoJohnson(all_numeric())
recipe_1 %>%
prep() %>%
juice() %>%
summary()
recipe_2 <-
recipe_base %>%
step_normalize(all_numeric())
recipe_2 %>%
prep() %>%
juice() %>%
summary()
recipe_2 <-
recipe_base %>%
step_normalize(all_numeric())
recipe_2 %>%
prep() %>%
juice() %>%
summary()
## 6.  Metrics
model_metrics <- yardstick::metric_set(f_meas, pr_auc)
## 7. K-Fold CV
data_penguins_3_cv_folds <-
rsample::vfold_cv(
v = 5,
data = training(penguins_split),
strata = species
)
## 8. 모델 학습(Model Training)
## 8 - 1. 일괄 작업 작성
ranger_r1_workflow <-
workflows::workflow() %>%
add_model(ranger_model) %>%
add_recipe(recipe_1)
glm_r2_workflow <-
workflows::workflow() %>%
add_model(glm_model) %>%
add_recipe(recipe_2)
xgboost_r2_workflow <-
workflows::workflow() %>%
add_model(xgboost_model) %>%
add_recipe(recipe_2)
## 8 - 2.  Gridsearch를 활용한 학습
tic("Ranger tune grid training duration ")
ranger_tuned <-
tune::tune_grid(
object = ranger_r1_workflow,
resamples = data_penguins_3_cv_folds,
grid = ranger_grid,
metrics = model_metrics,
control = tune::control_grid(save_pred = TRUE)
)
toc(log = TRUE)
tic("GLM tune grid training duration ")
glm_tuned <-
tune::tune_grid(
object = glm_r2_workflow,
resamples = data_penguins_3_cv_folds,
grid = glm_grid,
metrics = model_metrics,
control = tune::control_grid(save_pred = TRUE)
)
toc(log = TRUE)
tic("XGBoost tune grid training duration ")
xgboost_tuned <-
tune::tune_grid(
object = xgboost_r2_workflow,
resamples = data_penguins_3_cv_folds,
grid = xgboost_grid,
metrics = model_metrics,
control = tune::control_grid(save_pred = TRUE)
)
toc(log = TRUE)
#결과확인용
install.packages("finetune")
## 8 - 3. 학습 결과 확인
library(finetune)
tic("Tune race training duration ")
ft_xgboost_tuned <-
finetune::tune_race_anova(
object = xgboost_r2_workflow,
resamples = data_penguins_3_cv_folds,
grid = xgboost_grid,
metrics = model_metrics,
control = control_race(verbose_elim = TRUE) # 66
)
toc(log = TRUE)
plot_race(ft_xgboost_tuned) + labs(title = "Parameters Race by Fold")
# 결과
bind_cols(
tibble(model = c("Ranger", "GLM", "XGBoost")),
bind_rows(
ranger_tuned %>%
collect_metrics() %>% group_by(.metric) %>% summarise(best_va = max(mean, na.rm = TRUE)) %>% arranglm_tuned %>%
glm_tuned %>%
collect_metrics() %>% group_by(.metric) %>% summarise(best_va = max(mean, na.rm = TRUE)) %>% arranglm_tuned %>%
xgboost_tuned %>%
collect_metrics() %>% group_by(.metric) %>% summarise(best_va = max(mean, na.rm = TRUE)) %>% arran)
)
## 0. 패키지 불러오기
library(tidyverse)
## 1. 데이터 프레임 작성
# 파일 목록을 다 들고 와야 됨
files <- list.files(path = "data/nasdaq_stock/")
# 들고온 파일 목록을 다 읽어서, 데이터프레임
read_csv(paste0("data/nasdaq_stock/", files))
# 들고온 파일 목록을 다 읽어서, 데이터프레임
read_csv(paste0("data/nasdaq_stock/", files), id = "name")
# 들고온 파일 목록을 다 읽어서, 데이터프레임
read_csv(paste0("data/nasdaq_stock/", files), id = "name") %>% # %>% 이 기호 나오면 tidyverse 쓸거임
mutate(name = gsub("data/nasdaq_stock/", "", name))
# 들고온 파일 목록을 다 읽어서, 데이터프레임
read_csv(paste0("data/nasdaq_stock/", files), id = "name") %>% # %>% 이 기호 나오면 tidyverse 쓸거임
mutate(name = gsub("data/nasdaq_stock/", "", name),
name = gsub("\\.csv", "", name)) %>%
rename_with((tolower))
stocks
# 들고온 파일 목록을 다 읽어서, 데이터프레임
stocks <- read_csv(paste0("data/nasdaq_stock/", files), id = "name") %>% # %>% 이 기호 나오면 tidyverse 쓸거임
mutate(name = gsub("data/nasdaq_stock/", "", name),
name = gsub("\\.csv", "", name)) %>%
rename_with((tolower))
stocks
# 들고온 파일 목록을 다 읽어서, 데이터프레임
stocks <- read_csv(paste0("data/nasdaq_stock/", files), id = "name") %>% # %>% 이 기호 나오면 tidyverse 쓸거임
mutate(name = gsub("data/nasdaq_stock/", "", name),
name = gsub("\\.csv", "", name)) %>%
rename_with(tolower)
stocks
# 데이터 프레임을 결합
df <- read_csv("data/nasdaq_stock_names.csv")
df
stocks %>%
inner_join(df, by = c("name" = "stock_symbol"))
stocks <-
stocks %>%
inner_join(df, by = c("name" = "stock_symbol"))
stocks
View(stocks)
## 2. 시계열 데이터 시각화
stocks %>%
group_by(company)
## 2. 시계열 데이터 시각화
stocks %>%
group_by(company)
## 2. 시계열 데이터 시각화
(stocks %>%
group_by(company) %>%
filter(date == max(data)) %>%
arrange(-open) %>%
select(open, company))[c(1:3, 12:14),]
## 2. 시계열 데이터 시각화
(stocks %>%
group_by(company) %>%
filter(date == max(date)) %>%
arrange(-open) %>%
select(open, company))[c(1:3, 12:14),]
## 2. 시계열 데이터 시각화
(stocks %>%
group_by(company) %>%
filter(date == max(date)) %>%
arrange(-open) %>%  # 시작가 시준으로 sorting을 다시함
select(open, company)) #[c(1:3, 12:14),] #1~3번째, 12~14번째 가지고 올거얌
stocks %>%
ggplot(aes(date, open)) +
geom_line(ase(color = company))
stocks %>%
ggplot(aes(date, open)) +
geom_line(aes(color = company))
# 좀 더 해봐요!
stocks %>%
ggplot(aes(date, open)) +
geom_line(aes(color = company)) +
scale_y_continuous(sec.axis = sec_axis(~., breaks = end_labels$open,
labels = end_labels$company))
## 2. 시계열 데이터 시각화
end_labels <- (stocks %>%
group_by(company) %>%
filter(date == max(date)) %>%
arrange(-open) %>%  # 시작가 시준으로 sorting을 다시함
select(open, company))[c(1:3, 12:14),] #1~3번째, 12~14번째 가지고 올거얌
# 좀 더 해봐요!
stocks %>%
ggplot(aes(date, open)) +
geom_line(aes(color = company)) +
scale_y_continuous(sec.axis = sec_axis(~., breaks = end_labels$open,
labels = end_labels$company))
## 0. 패키지 불러오기
library(tidyverse)
## 1. 데이터 프레임 작성
# 파일 목록을 다 들고 와야 됨
files <- list.files(path = "data/nasdaq_stock/")
# 들고온 파일 목록을 다 읽어서, 데이터프레임
stocks <- read_csv(paste0("data/nasdaq_stock/", files), id = "name") %>% # %>% 이 기호 나오면 tidyverse 쓸거임
mutate(name = gsub("data/nasdaq_stock/", "", name),
name = gsub("\\.csv", "", name)) %>%
rename_with(tolower)
stocks
# 데이터 프레임을 결합
df <- read_csv("data/nasdaq_stock_names.csv")
df
stocks <-
stocks %>%
inner_join(df, by = c("name" = "stock_symbol"))
## 2. 시계열 데이터 시각화
end_labels <- (stocks %>%
group_by(company) %>%
filter(date == max(date)) %>%
arrange(-open) %>%  # 시작가 시준으로 sorting을 다시함
select(open, company))[c(1:3, 12:14),] #1~3번째, 12~14번째 가지고 올거얌
# 좀 더 해봐요!
stocks %>%
ggplot(aes(date, open)) +
geom_line(aes(color = company)) +
scale_y_continuous(sec.axis = sec_axis(~., breaks = end_labels$open,
labels = end_labels$company))
# 좀 더 해봐요!
stocks %>%
ggplot(aes(date, open)) +
geom_line(aes(color = company)) +
scale_y_continuous(sec.axis = sec_axis(~., breaks = end_labels$open,
labels = end_labels$company)) + #우리가 선택한 6개의 종가를 표시해줘
scale_x_date(expand = c(0,0)) +
labs(x = "", y = "Open", color = "", title = "주요 회사의 시작가격") +
theme(legend.position = "none")
(stocks %>%
filter(company %in% end_labels$company[1:3]) %>%
ggplot(aes(date, open)) +
geom_line(aes(color = company)) +
facet_wrap(~company) +
theme_bw() +
theme(legend.position = "none") +
labs(title = "Top 3", x = ""))
(stocks %>%
filter(company %in% end_labels$company[-(1:3)]) %>%
ggplot(aes(date, open)) +
geom_line(aes(color = company)) +
facet_wrap(~company) +
theme_bw() +
theme(legend.position = "none") +
labs(title = "Bottom 3", x = ""))
#시계열
stocks %>%
filter(name = "AAPL") %>%
select(ds = date, y = open) #x가 고정이다!! 종가 대신 시작가를 정한 것! 여러개해도 괜츈 벋 우리가 그걸 할 수 있는 능력이 안됨
#시계열
stocks %>%
filter(name == "AAPL") %>%
select(ds = date, y = open) #x가 고정이다!! 종가 대신 시작가를 정한 것! 여러개해도 괜츈 벋 우리가 그걸 할 수 있는 능력이 안됨
## 0. 패키지 불러오기
library(tidyverse)
library(lubridate)
library(scales)
library(patchwork)
library(corrr)
library(rstatix)
library(prophet)
library(astsa)
library(forecast)
install.packages("corrr")
install.packages("patchwork")
install.packages("rstatix")
install.packages("prophet")
install.packages("astsa")
install.packages("forecast")
library(scales)
library(patchwork)
library(corrr)
library(rstatix)
library(prophet)
library(astsa)
library(forecast)
(stocks %>%
filter(company %in% end_labels$company[1:3]) %>%
ggplot(aes(date, open)) +
geom_line(aes(color = company)) +
facet_wrap(~company) +
theme_bw() +
theme(legend.position = "none") +
labs(title = "Top 3", x = ""))
(stocks %>%
filter(company %in% end_labels$company[-(1:3)]) %>%
ggplot(aes(date, open)) +
geom_line(aes(color = company)) +
facet_wrap(~company) +
theme_bw() +
theme(legend.position = "none") +
labs(title = "Bottom 3", x = ""))
## 1. 데이터 프레임 작성
# 파일 목록을 다 들고 와야 됨
files <- list.files(path = "data/nasdaq_stock/")
# 들고온 파일 목록을 다 읽어서, 데이터프레임
stocks <- read_csv(paste0("data/nasdaq_stock/", files), id = "name") %>% # %>% 이 기호 나오면 tidyverse 쓸거임
mutate(name = gsub("data/nasdaq_stock/", "", name),
name = gsub("\\.csv", "", name)) %>%
rename_with(tolower)
## 1. 데이터 프레임 작성
# 파일 목록을 다 들고 와야 됨
files <- list.files(path = "data/nasdaq_stock/")
## 1. 데이터 프레임 작성
# 파일 목록을 다 들고 와야 됨
files <- list.files(path = "data/nasdaq_stock/")
# 들고온 파일 목록을 다 읽어서, 데이터프레임
stocks <- read_csv(paste0("data/nasdaq_stock/", files), id = "name") %>% # %>% 이 기호 나오면 tidyverse 쓸거임
mutate(name = gsub("data/nasdaq_stock/", "", name),
name = gsub("\\.csv", "", name)) %>%
rename_with(tolower)
